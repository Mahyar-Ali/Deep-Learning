{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of a Deep Neural Network\n",
    "### Note:-\n",
    "- Number of Hidden Layers = 2\n",
    "- Output Layer size = 6\n",
    "- This NN was deigned to recognize Numbers from 0 to 5 illustrated by using Fingers\n",
    "- Optimization Function used is AdamOptimizer\n",
    "- NN is trained using minibatches\n",
    "- To evaluate performance, Train and Test sets are required\n",
    "date : March 29,2020\n",
    "author: m.mahyar_ali"
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#importing all Libraries\n",
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Creating Place Holder for X and Y so that we can change them at test time. Changing is required when using mini batches.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_placeholders(n_x,n_y):\n",
    "    X = tf.placeholder(dtype=tf.float32,shape=[n_x,None],name = 'X')\n",
    "    Y = tf.placeholder(dtype=tf.float32,shape=[n_y,None],name = 'Y')\n",
    "    return X,Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**Initialize all the weights and biases. Xavier Initializer is used to overcome Gradient Exploding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x):\n",
    "    tf.set_random_seed(1)\n",
    "    parameters = {}\n",
    "    parameters['W1'] = tf.get_variable('W1',[25,n_x],initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    parameters['b1'] = tf.get_variable('b1',[25,1],initializer=tf.zeros_initializer())\n",
    "    parameters['W2'] = tf.get_variable('W2',[12,25],initializer= tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    parameters['b2'] = tf.get_variable('b2',[12,1],initializer = tf.zeros_initializer())\n",
    "    parameters['W3'] = tf.get_variable('W3',[6,12],initializer = tf.contrib.layers.xavier_initializer(seed=1))\n",
    "    parameters['b3'] = tf.get_variable('b3',[6,1],initializer = tf.zeros_initializer())\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feed forward in the NN to compute Z3.Note:A3 is not required to compute because the cost function automatically computes it** <br>\n",
    "**Required : X=Dataset , parameters = Dictionary of Weights and Biases**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,parameters):\n",
    "    W1 = parameters['W1']\n",
    "    b1 = parameters['b1']\n",
    "    W2 = parameters['W2']\n",
    "    b2 = parameters['b2']\n",
    "    W3 = parameters['W3']\n",
    "    b3 = parameters['b3']\n",
    "    \n",
    "    Z1 = tf.add(tf.matmul(W1,X),b1)\n",
    "    A1 = tf.nn.relu(Z1)\n",
    "    Z2 = tf.add(tf.matmul(W2,A1),b2)\n",
    "    A2 = tf.nn.relu(Z2)\n",
    "    Z3 = tf.add(tf.matmul(W3,A2),b3)\n",
    "    \n",
    "    return Z3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To compute the cost. For this Example softmax Classifier is used **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def Cost(Z3,Y):\n",
    "    logits = tf.transpose(Z3)\n",
    "    labels = tf.transpose(Y)\n",
    "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=labels))\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Create Random Mini Batches of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_mini_batches(X_train,Y_train,minibatch_size,seed=0):\n",
    "    np.random.seed(seed) \n",
    "    m = X_train.shape[1]\n",
    "    random_perm = list(np.random.permutation(m))\n",
    "    X_train_S = X_train[:,random_perm]\n",
    "    Y_train_S = Y_train[:,random_perm]\n",
    "    num_minibatches = m//minibatch_size\n",
    "    minibatches = []\n",
    "    for batch in range(num_minibatches):\n",
    "        minibatch_X = X_train_S[:,minibatch_size*batch:minibatch_size*(batch+1)]\n",
    "        minibatch_Y = Y_train_S[:,minibatch_size*batch:minibatch_size*(batch+1)]\n",
    "        minibatches.append((minibatch_X,minibatch_Y))\n",
    "        \n",
    "    if m%minibatch_size != 0 :\n",
    "        minibatch_X = X_train_S[:,minibatch_size*num_minibatches:]\n",
    "        minibatch_Y = Y_train_S[:,minibatch_size*num_minibatches:]\n",
    "        minibatches.append((minibatch_X,minibatch_Y))\n",
    "        \n",
    "    return minibatches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Putting all the pieces in one place. Initializing all the variables and then running the optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train,Y_train,X_test,Y_test,learning_rate=0.0001,num_epochs = 1500,minibatch_size=32,print_cost=True):\n",
    "    ops.reset_default_graph()\n",
    "    seed = 3  \n",
    "    n_x = X_train.shape[0]\n",
    "    n_y = Y_train.shape[0]\n",
    "    m = X_train.shape[1]\n",
    "    X,Y = create_placeholders(n_x,n_y)\n",
    "    \n",
    "    parameters = initialize_parameters(n_x)\n",
    "    \n",
    "    Z3 = forward_propagation(X,parameters)\n",
    "    \n",
    "    cost = Cost(Z3,Y)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init)\n",
    "        for epoch in range(num_epochs):\n",
    "            epoch_cost = 0\n",
    "            num_minibatches = int(m/minibatch_size)\n",
    "            seed=seed+1\n",
    "            minibatches = random_mini_batches(X_train,Y_train,minibatch_size,seed)\n",
    "            for minibatch in minibatches:\n",
    "                (minibatch_X, minibatch_Y) = minibatch\n",
    "                _,minibatch_cost = sess.run([optimizer,cost],feed_dict = {X:minibatch_X,Y:minibatch_Y})\n",
    "                \n",
    "                epoch_cost += minibatch_cost/minibatch_size\n",
    "            if (epoch%100==0):\n",
    "                print(\"Cost after epoch \"+str(epoch)+\" is \"+ str(epoch_cost))\n",
    "    \n",
    "        parameters = sess.run(parameters)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
