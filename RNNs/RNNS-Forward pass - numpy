
#Simple RNN

def rnn_cell_forward(xt,a_prev,parameters):
    
    #Retrieve weights and biases from the parameters
    Wax = parameters["Wax"]
    Waa = parameters["Waa"]
    Wya = parameters["Wya"]
    ba = parameters["ba"]
    by = parameters["by"]
    
    #Compute a_next and yt_pred
    a_next = np.tanh(np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba)
    yt_pred = softmax(np.dot(Wya,a_next)+by)
    
    #Cache all the necessary variables
    cache = (a_next, a_prev, xt, parameters)
    
    return a_next,yt_pred,caches
    

def rnn_forward(x, a0, parameters):
    
    #Get the needed dimensions
    n_x,m,T_x = x.shape
    n_y, n_a = parameters["Wya"].shape
    
    # initialize "a" and "y_pred" with zeros
    a = np.zeros([n_a,m,T_x])
    y_pred = np.zeros([n_y,m,T_x])
    
    #To keep track of the computations at each step
    caches =[]
    
    a_next = a_0
    
    for t in range(T_x):
        
        xt = [:,:,t]
        
        a_next,yt_pred,caches = rnn_cell_forward(xt,a_next,parameters)
        
        #Store the activations and output at each time step
        a[:,:,t] = a_next
        y_pred[:,:,t] = yt_pred
        
        caches.append(cache)
        
    caches = (caches, x)
    
    return a, y_pred, caches
